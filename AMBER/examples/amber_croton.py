# -*- coding: utf-8 -*-
"""amber-branched.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M7ZgLyztNCPGBi5N-2uUN-380MZq7aAI

# AMBER branched model
"""

# Commented out IPython magic to ensure Python compatibility.
#@title Install and Import modules

# %tensorflow_version 1.x
import tensorflow as tf
from tensorflow.keras.utils import Sequence
from tensorflow.keras.optimizers import SGD, Adam
#!python -m pip install -q git+https://github.com/zj-zhang/AMBER@ac839aebcceb1a61c1664ab19bb8d681228232f3
#!python -m pip install -q keras==2.2.5
import os
import sys
import h5py
import pandas as pd
import numpy as np
from amber.architect.modelSpace import BranchedModelSpace, ModelSpace, Operation
from amber import Amber


#@title Dataset Split, Generator, Pearsons Loss Functions
def dataset_split(set_, train_prop, test_prop): #set_ = 'train', 'test', or 'val'
    df = pd.read_csv('./data/final_df.csv')
    df['genename_num'] = df['genename'].str.cat(df['Number'].astype(str),sep="-")
    samp_lst = df['genename_num'].tolist()
    np.random.seed(115)
    np.random.shuffle(samp_lst)

    num_test = int(len(samp_lst)*train_prop)    
    train, test_val = samp_lst[:num_test], samp_lst[num_test::]

    test_prop /= (1 - train_prop)
    num_test = int(len(test_val)*test_prop)
    test, val = test_val[:num_test], test_val[num_test::]
    
    if set_ == 'train':
        return train
    if set_ == 'val':
        return val
    if set_ == 'test':
        return test


class CrisprGenerator(Sequence):
    def __init__(self, ref_store, out_store, samp_list, minproba=1):
        self.ref_store = ref_store
        self.out_store = out_store
        self.samp_list = samp_list
        self.minproba = minproba

    def __getitem__(self, item):
        samp_id = self.samp_list[item]
        x_left = self.ref_store[samp_id]['x_left']
        x_right = self.ref_store[samp_id]['x_right']
        x_out = self.out_store[samp_id]['x_out']
        proba = self.out_store[samp_id]['proba']
        # Filter by min proba.
        proba_inds = np.where(proba > self.minproba)
        x_out = x_out[proba_inds]
        proba = proba[proba_inds]
        total = x_out.shape[0]
        x_left = np.dstack([x_left]*total)
        x_right = np.dstack([x_right]*total)
        x_left = np.transpose(x_left, (2, 0, 1))
        x_right = np.transpose(x_right, (2, 0, 1))
        return [x_left, x_right, x_out], proba/100.

    def __len__(self):
        return len(self.samp_list)


def batched_pearson_loss(y_true, y_pred):
    """Custom loss function for computing negative Pearson correlation within
    each batch.

    Parameters
    ----------
    y_true : tf.Tensor
        ground-truth; expect shape to be (None, 1).
    y_pred : tf.Tensor
        predicted values; expect shape to be (None, 1). The prediction `y_pred` is the linear part of the
        softmax function, and the correlation will be calculated *after* softmax transformation.

    Returns
    -------
    tf.Tensor : the computed loss
        negative pearson correlation coefficient

    Notes
    -------
    Need to be careful if y_pred is a constant, say, all zeros. For now, defines 0/0:=0.

    """
    x = y_true
    y = tf.exp(y_pred)
    y = y / tf.reduce_sum(y)
    # y = y_pred
    x_mean = tf.reduce_mean(x)
    y_mean = tf.reduce_mean(y)
    r_num = tf.reduce_sum((x - x_mean)*(y - y_mean))
    r_denom_x = tf.sqrt(tf.reduce_sum((x - x_mean)**2))
    r_denom_y = tf.sqrt(tf.reduce_sum((y - y_mean)**2))
    r_denom = r_denom_x * r_denom_y
    r = tf.math.divide_no_nan(r_num, r_denom)
    loss = r * -1
    return loss

#@title Download Data
#! if [ ! -f final_df.csv ]; then wget https://sourceforge.net/projects/v-summer-research-20/files/SPROUT_specific_out/final_df.csv > /dev/null 2>&1; fi
#! if [ ! -f refseq.hdf5 ]; then wget https://sourceforge.net/projects/v-summer-research-20/files/SPROUT_specific_out/refseq.hdf5 > /dev/null 2>&1; fi
#! if [ ! -f specific_out.hdf5 ]; then wget https://sourceforge.net/projects/v-summer-research-20/files/SPROUT_specific_out/specific_out.hdf5.zip > /dev/null 2>&1; unzip specific_out.hdf5.zip; rm specific_out.hdf5.zip; fi


def get_branch_ms(ns):
    branch_ms = ModelSpace.from_dict([
        # layer 1
        [
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 12, 'activation': 'relu', 'name': '%s_L1_relu12' % ns},
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 12, 'activation': 'tanh', 'name': '%s_L1_tanh12' % ns},
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 12, 'activation': 'sigmoid', 'name': '%s_L1_sigmoid12' % ns},
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 2, 'activation': 'relu', 'name': '%s_L1_relu2' % ns},
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 2, 'activation': 'tanh', 'name': '%s_L1_tanh2' % ns},
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 2, 'activation': 'sigmoid', 'name': '%s_L1_sigmoid2' % ns}
        ],
        # layer 2
        [
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 12, 'activation': 'relu', 'name': '%s_L2_relu12' % ns},
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 12, 'activation': 'tanh', 'name': '%s_L2_tanh12' % ns},
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 12, 'activation': 'sigmoid', 'name': '%s_L2_sigmoid12' % ns},
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 2, 'activation': 'relu', 'name': '%s_L2_relu2' % ns},
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 2, 'activation': 'tanh', 'name': '%s_L2_tanh2' % ns},
            {'Layer_type': 'conv1d', 'filters': 32, 'kernel_size': 2, 'activation': 'sigmoid', 'name': '%s_L2_sigmoid2' % ns},

            {'Layer_type': 'identity', 'name': '%s_L2_id' % ns}
        ],
        # layer 3
        [
            {'Layer_type': 'maxpool1d', 'pool_size': 2, 'strides': 2, 'name': '%s_L3_maxp2' % ns},
            {'Layer_type': 'avgpool1d', 'pool_size': 2, 'strides': 2, 'name': '%s_L3_avgp2' % ns},
            # {'Layer_type': 'maxpool1d', 'pool_size': 4, 'strides': 4, 'name': '%s_L3_maxp4' % ns},
            # {'Layer_type': 'avgpool1d', 'pool_size': 4, 'strides': 4, 'name': '%s_L3_avgp4' % ns},
            {'Layer_type': 'identity', 'name': '%s_L3_id' % ns}
        ],

        # layer 4
        [
            {'Layer_type': 'dropout', 'rate': 0.2, 'name': '%s_L4_drop0.2' % ns},
            {'Layer_type': 'dropout', 'rate': 0.4, 'name': '%s_L4_drop0.4' % ns},
            # {'Layer_type': 'dropout', 'rate': 0.6, 'name': '%s_L4_drop0.6' % ns},
            {'Layer_type': 'identity', 'name': '%s_L4_id' % ns}
        ],
        # layer 5
        [
            {'Layer_type': 'flatten', 'name': '%s_L5_flat' % ns},
            # {'Layer_type': 'globalmaxpool1d', 'name': '%s_L5_gbmax' % ns},
            {'Layer_type': 'globalavgpool1d', 'name': '%s_L5_gbavg' % ns},
            # {'Layer_type': 'lstm', 'units': 128, 'name': '%s_L5_lstm' % ns}
        ]


    ])
    return branch_ms


stem_ms = ModelSpace.from_dict([
    # Layer 1
    [
        {'Layer_type': 'dense', 'units': 32, 'activation': 'relu', 'name': 'stem_u32_relu'},
        # {'Layer_type': 'dense', 'units': 32, 'activation': 'tanh', 'name': 'stem_u32_tanh'},
        # {'Layer_type': 'dense', 'units': 32, 'activation': 'sigmoid', 'name': 'stem_u32_sigmoid'},
        {'Layer_type': 'dense', 'units': 64, 'activation': 'relu', 'name': 'stem_u64_relu'},
        # {'Layer_type': 'dense', 'units': 64, 'activation': 'tanh', 'name': 'stem_u64_tanh'},
        # {'Layer_type': 'dense', 'units': 64, 'activation': 'sigmoid', 'name': 'stem_u64_sigmoid'}
    ],
    # Layer 2
    [
        {'Layer_type': 'dense', 'units': 32, 'activation': 'relu', 'name': 'stem2_u32_relu'},
        # {'Layer_type': 'dense', 'units': 32, 'activation': 'tanh', 'name': 'stem2_u32_tanh'},
        # {'Layer_type': 'dense', 'units': 32, 'activation': 'sigmoid', 'name': 'stem2_u32_sigmoid'},
        {'Layer_type': 'dropout', 'rate': 0.2, 'name': 'stem2_drop0.2'},
        {'Layer_type': 'dropout', 'rate': 0.4, 'name': 'stem2_drop0.4'},
        # {'Layer_type': 'dropout', 'rate': 0.6, 'name': 'stem2_drop0.6'},
        {'Layer_type': 'identity', 'name': 'stem2_id'}
    ],
    # Layer 3; i.e., the actual output layer
    [
        {'Layer_type': 'dense', 'units': 1, 'activation': 'linear', 'name': 'output_linear'},
        {'Layer_type': 'dense', 'units': 1, 'activation': 'sigmoid', 'name': 'output_sigmoid'},
        {'Layer_type': 'dense', 'units': 1, 'activation': 'tanh', 'name': 'output_tanh'}
    ]
])

bms_left = get_branch_ms('LF')
bms_right = get_branch_ms('RT')
bms_alt = get_branch_ms('ALT')

ms = BranchedModelSpace(subspaces=[
    [bms_left, bms_right, bms_alt],
    stem_ms
])
print(str(ms))

if len(sys.argv) > 1:
    wd = sys.argv[1]
else:
    wd = "./outputs/amber_croton/"
print("working dir: %s" % wd)
os.makedirs(wd, exist_ok=True)
inputs_op = [
    Operation('input', shape=(30, 4), name='LF_seq'),
    Operation('input', shape=(30, 4), name='RT_seq'),
    Operation('input', shape=(80, 4), name='ALT_seq'),
]
output_op = Operation('identity', name='pseudo_output')
model_compile_dict = {
    'optimizer': 'adam',
    #'optimizer': SGD(lr=0.01, momentum=0.9, decay=1e-5, nesterov=True),
    'loss': batched_pearson_loss
}

# enable embedding sharing between branches
layer_embedding_sharing = {len(bms_left)*k+i: i for i in range(len(bms_left)) for k in range(1, 3)}


hf_ref = h5py.File('./data/refseq.hdf5', 'r')
hf_out = h5py.File('./data/specific_out.hdf5', 'r')
ref_store = {
    k : {
        'x_left': hf_ref['%s/x_left' % k][()],
        'x_right': hf_ref['%s/x_right' % k][()]
         }
    for k in hf_ref
}
out_store = {
    k: {
        'x_out': hf_out['%s/x_out' % k][()],
        'proba': hf_out['%s/proba' % k][()]
         }
    for k in hf_out
}
train_samps = dataset_split(set_='train', train_prop=0.8, test_prop=0.1)
val_samps = dataset_split(set_='val', train_prop=0.8, test_prop=0.1)

train_gen = CrisprGenerator(ref_store, out_store, train_samps)
val_gen = CrisprGenerator(ref_store, out_store, val_samps)


# First, define the components we need to use
type_dict = {
    'controller_type': 'GeneralController',
    'modeler_type': 'KerasBranchModelBuilder',
    'knowledge_fn_type': 'zero',
    'reward_fn_type': 'LossReward',
    'manager_type': 'GeneralManager',
    'env_type': 'ControllerTrainEnv'
}

specs = {
    'model_space': ms,
    
    'controller': {
        'share_embedding': layer_embedding_sharing,
        'with_skip_connection': False,
        'lstm_size': 64,
        'lstm_num_layers': 1,
        'kl_threshold': 0.01,
        'train_pi_iter': 100,
        'optim_algo': 'adam',
        'temperature': 1.5,
        'lr_init': 0.001,
        'tanh_constant': 2.0,
        'buffer_size': 5,
        'batch_size': 5
    },

    'model_builder': {
        'inputs_op': inputs_op,
        'outputs_op': [output_op],
        'model_compile_dict': model_compile_dict,
        'with_bn': False
    },

    'knowledge_fn': {'data': None, 'params': {}},

    'reward_fn': {},

    'manager': {
        'data': {
            'train_data': train_gen,
            'validation_data': val_gen
        },
        'params': {
            'epochs': 50,
            'child_batchsize': None,
            'fit_kwargs': {
                'earlystop_patience': 10,
                'workers': 5,
                'max_queue_size': 200
                # 'steps_per_epoch': 1580,
                # 'validation_steps': 198
            },
            'predict_kwargs': {
                # 'steps': 198,
                # 'use_multiprocessing': True
            },
            'store_fn': 'model_plot',
            'working_dir': wd,
            'verbose': 2
        }
    },

    'train_env': {
        'max_episode': 150,
        'max_step_per_ep': 3,
        'working_dir': wd,
        'time_budget': "48:00:00",
        'with_skip_connection': False,
        'save_controller': False
    }
}

amb = Amber(types=type_dict, specs=specs)

amb.run()

