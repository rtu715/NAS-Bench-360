Neural net backbone for 2D data - NAS using XD-operations 

Our generalized backbone: 
-begin and end with customzible conv/linear layers
-Our search space: intermediate blocks, number of blocks is a hyperparameter and can be tuned...
-Block configurations:
 -inception-like modules
 -wide-resnet
 -densenet 
-relus and batchnorms used as usual


CIFAR10: ResNet20
-starting conv
-variable (3) layers with a numbers of basic blocks (3)
-each block = 2 conv layers + bn + relus + skip connect
-final linear layer
-continue to downsample with strides


PDEs: FNO (Conv1d + 2d)
-starting linear layer
-variable layers (4) of 1D + 2D convs (like an inception module?)
-each layer = conv concatenated + bn + relu activation 
-two final linear layers and relu in between 
-no change of dimensions (?)


Other applications...
making use of dilated convolutions? - larger receptive field, as a replacement of max pooling? 