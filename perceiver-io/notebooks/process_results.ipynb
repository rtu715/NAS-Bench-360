{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate test scores for each run on each dataset here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_results(logfile):\n",
    "    perf = {}\n",
    "    with open(logfile, 'r') as f:\n",
    "        prevline = ''\n",
    "        for line in f.readlines():\n",
    "            if 'DATALOADER:0 TEST RESULTS' in prevline:\n",
    "                perf = eval(line)\n",
    "            prevline = line\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [70.660001039505, 69.69000101089478, 69.760000705719]\n",
      "mean:   70.03666758537292\n",
      "std:    0.4416887558745739\n"
     ]
    }
   ],
   "source": [
    "metric = 'test_acc'\n",
    "logs = [f'../logs/cifar100/version_{i}/test.log' for i in range(3)]\n",
    "perfs = [(1 - get_test_results(log)[metric]) * 100 for log in logs]\n",
    "print('scores:', perfs)\n",
    "print('mean:  ', np.mean(perfs))\n",
    "print('std:   ', np.std(perfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spherical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [82.41000026464462, 82.84000009298325, 82.46999979019165]\n",
      "mean:   82.5733333826065\n",
      "std:    0.19014614832426663\n"
     ]
    }
   ],
   "source": [
    "metric = 'test_acc'\n",
    "logs = [f'../logs/spherical/version_{i}/test.log' for i in range(3)]\n",
    "perfs = [(1 - get_test_results(log)[metric]) * 100 for log in logs]\n",
    "print('scores:', perfs)\n",
    "print('mean:  ', np.mean(perfs))\n",
    "print('std:   ', np.std(perfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NinaPro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [23.181819915771484, 19.69696879386902, 23.787879943847656]\n",
      "mean:   22.222222884496052\n",
      "std:    1.8026847304468503\n"
     ]
    }
   ],
   "source": [
    "metric = 'test_acc'\n",
    "logs = [f'../logs/ninapro/version_{i}/test.log' for i in range(3)]\n",
    "perfs = [(1 - get_test_results(log)[metric]) * 100 for log in logs]\n",
    "print('scores:', perfs)\n",
    "print('mean:  ', np.mean(perfs))\n",
    "print('std:   ', np.std(perfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.3272858262062073, 0.3328063488006592, 0.34655439853668213]\n",
      "mean:   0.33554885784784955\n",
      "std:    0.008101871669317157\n"
     ]
    }
   ],
   "source": [
    "metric = 'test_f1_macro'\n",
    "logs = [f'../logs/ecg/version_{i}/test.log' for i in range(3)]\n",
    "perfs = [get_test_results(log)[metric] for log in logs]\n",
    "print('scores:', perfs)\n",
    "print('mean:  ', np.mean(perfs))\n",
    "print('std:   ', np.std(perfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [15.916997194290161, 15.842002630233765, 16.044002771377563]\n",
      "mean:   15.93433419863383\n",
      "std:    0.08337242967647418\n"
     ]
    }
   ],
   "source": [
    "metric = 'test_acc'\n",
    "logs = [f'../logs/satellite/version_{i}/test.log' for i in range(3)]\n",
    "perfs = [(1 - get_test_results(log)[metric]) * 100 for log in logs]\n",
    "print('scores:', perfs)\n",
    "print('mean:  ', np.mean(perfs))\n",
    "print('std:   ', np.std(perfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.6207776665687561, 0.611752986907959, 0.6143784523010254]\n",
      "mean:   0.6156363685925802\n",
      "std:    0.0037901605764066233\n"
     ]
    }
   ],
   "source": [
    "metric = 'test_auroc'\n",
    "logs = [f'../logs/deepsea/version_{i}/test.log' for i in range(3)]\n",
    "perfs = [get_test_results(log)[metric] for log in logs]\n",
    "print('scores:', perfs)\n",
    "print('mean:  ', np.mean(perfs))\n",
    "print('std:   ', np.std(perfs)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darcy Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.2429637461900711, 0.23058705031871796, 0.2371530532836914]\n",
      "mean:   0.23690128326416016\n",
      "std:    0.005055900268006586\n"
     ]
    }
   ],
   "source": [
    "metric = 'test_LpLoss'\n",
    "logs = [f'../logs/darcyflow/version_{i}/test.log' for i in range(3)]\n",
    "perfs = [get_test_results(log)[metric] for log in logs]\n",
    "print('scores:', perfs)\n",
    "print('mean:  ', np.mean(perfs))\n",
    "print('std:   ', np.std(perfs)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSICOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [8.149910573235557, 7.995219983072598, 8.047055101791857]\n",
      "mean:   8.064061886033336\n",
      "std:    0.06428694892909619\n"
     ]
    }
   ],
   "source": [
    "# psicov\n",
    "metric = 'test_mae8'\n",
    "logs = [f'../logs/psicov/version_{i}/test.log' for i in range(3)]\n",
    "perfs = [get_test_results(log)[metric] for log in logs]\n",
    "print('scores:', perfs)\n",
    "print('mean:  ', np.mean(perfs))\n",
    "print('std:   ', np.std(perfs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c38ddd05bf56706ba4389aebe33b9e08a1ad4d8f8566ec40e1e11ab73ad1579c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('perceiver-io')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
